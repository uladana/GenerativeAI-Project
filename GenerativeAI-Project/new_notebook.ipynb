{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043610bc",
   "metadata": {},
   "source": [
    "## Sprachmodell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7386974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import wandb\n",
    "import requests\n",
    "import math\n",
    "import traceback\n",
    "from huggingface_hub import login, HfApi\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------ Configuración ------------------ #\n",
    "# 1. Configuración del Modelo y Entrenamiento\n",
    "class Config:\n",
    "    # Arquitectura del Modelo\n",
    "    d_model = 256          # Dimensión de los embeddings\n",
    "    n_head = 4             # Número de heads de atención\n",
    "    num_layers = 3         # Número de capas Transformer\n",
    "    max_seq_len = 64      # Longitud máxima de secuencia\n",
    "    \n",
    "    # Hiperparámetros de Entrenamiento\n",
    "    batch_size = 16        # Tamaño del batch\n",
    "    lr = 3e-4              # Tasa de aprendizaje\n",
    "    epochs = 4             # Número de épocas\n",
    "    grad_clip = 1.0        # Clipping de gradientes\n",
    "    \n",
    "    # Configuración de Dispositivo\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    \n",
    "    # Datos y Modelo\n",
    "    dataset_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    model_name = \"tiny-shakespeare-transformer\"\n",
    "    hf_username = \"uladana\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec4f08",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dataset y Tokenización\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Tokenizar todo el texto\n",
    "        tokens = tokenizer.encode(text, \n",
    "                         add_special_tokens=False, \n",
    "                         truncation=True, \n",
    "                         max_length=1024)[:10000]  # Limitar a 10k tokens para seguridad\n",
    "        \n",
    "        # Dividir en secuencias de longitud seq_length+1 (input+target)\n",
    "        self.samples = []\n",
    "        for i in range(0, len(tokens) - seq_length, seq_length):\n",
    "            self.samples.append(tokens[i:i+seq_length+1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.samples[idx]\n",
    "        input_seq = torch.tensor(sequence[:-1], dtype=torch.long)  # Todos menos el último\n",
    "        target_seq = torch.tensor(sequence[1:], dtype=torch.long)  # Todos menos el primero\n",
    "        return input_seq, target_seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c1168",
   "metadata": {},
   "source": [
    "## Modell Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vocab_size = 50257  # Tamaño del vocabulario de GPT-2\n",
    "        \n",
    "        # Capa de embedding de tokens\n",
    "        self.token_embedding = nn.Embedding(vocab_size, Config.d_model)\n",
    "        \n",
    "        # Capa de embedding posicional\n",
    "        self.position_embedding = nn.Embedding(Config.max_seq_len, Config.d_model)\n",
    "        \n",
    "        # Capas Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=Config.d_model,\n",
    "            nhead=Config.n_head,\n",
    "            dim_feedforward=Config.d_model*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=Config.num_layers)\n",
    "        \n",
    "        # Capa de salida\n",
    "        self.output = nn.Linear(Config.d_model, vocab_size)\n",
    "        \n",
    "        # Inicialización de pesos\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # Crear máscara de atención triangular superior\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(Config.device)\n",
    "        \n",
    "        # Embeddings de tokens + posiciones\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.long, device=Config.device).unsqueeze(0)\n",
    "        pos_embeds = self.position_embedding(positions)\n",
    "        x = token_embeds + pos_embeds\n",
    "        \n",
    "        # Pasar por el decoder\n",
    "        x = self.decoder(x, x, tgt_mask=mask)\n",
    "        \n",
    "        # Salida final\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b8917c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec38582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Inicializar Weights & Biases\n",
    "    wandb.init(\n",
    "        project=\"ki-projekt-einfach\",\n",
    "        config={\n",
    "            \"architecture\": \"TransformerDecoder\",\n",
    "            \"dataset\": \"TinyShakespeare\",\n",
    "            **{k:v for k,v in vars(Config).items() if not k.startswith('_')}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Cargar y preparar los datos\n",
    "    print(\"Cargando y preparando datos...\")\n",
    "    try:\n",
    "        text_data = requests.get(Config.dataset_url, timeout=10).text\n",
    "    except Exception as e:\n",
    "        print(f\"Error al descargar el dataset: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Inicializar tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Dividir en train y validation\n",
    "    split_idx = int(len(text_data) * 0.9)  # 90% train, 10% validation\n",
    "    train_data = text_data[:split_idx]\n",
    "    val_data = text_data[split_idx:]\n",
    "    \n",
    "    # Crear datasets y dataloaders\n",
    "    train_dataset = TextDataset(train_data, tokenizer, Config.max_seq_len)\n",
    "    val_dataset = TextDataset(val_data, tokenizer, Config.max_seq_len)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=Config.batch_size)\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset)} | Val samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Inicializar modelo y optimizador y contar parámetros\n",
    "    model = LanguageModel().to(Config.device)\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"El modelo tiene {param_count:,} parámetros totales\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Entrenamiento\n",
    "    print(\"Comenzando entrenamiento...\")\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(Config.epochs):\n",
    "        # Fase de entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        # Crear barra de progreso para los batches\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.epochs} [Train]\")\n",
    "\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs, targets = inputs.to(Config.device), targets.to(Config.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clipping de gradientes\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), Config.grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Actualizar barra de progreso            \n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Fase de validación\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{Config.epochs} [Val]\")\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_progress:\n",
    "                inputs, targets = inputs.to(Config.device), targets.to(Config.device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Actualizar barra de progreso de validación\n",
    "                val_progress.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Registrar métricas\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"lr\": optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{Config.epochs}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Generar ejemplo de texto cada 2 épocas o en la última\n",
    "        if (epoch + 1) % 2 == 0 or epoch == Config.epochs - 1:\n",
    "            example_prompt = \"ROMEO:\"\n",
    "            example_text = generate_text(model, tokenizer, example_prompt, max_length=50)\n",
    "            print(f\"\\nTexto generado (época {epoch+1}):\")\n",
    "            print(f\"Prompt: {example_prompt}\")\n",
    "            print(f\"Generado: {example_text}\\n\")\n",
    "            \n",
    "            # Logear el ejemplo generado en wandb\n",
    "            wandb.log({\"generated_example\": wandb.Html(f\"<p><strong>Prompt:</strong> {example_prompt}</p><p>{example_text}</p>\")})\n",
    "        \n",
    "        # Guardar el mejor modelo\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(\"¡Mejor modelo guardado!\")\n",
    "    \n",
    "    # Guardar el modelo final\n",
    "    torch.save(model.state_dict(), \"final_model.pt\")\n",
    "    print(\"Entrenamiento completado.\")\n",
    "    \n",
    "    return model, tokenizer, best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f3453",
   "metadata": {},
   "source": [
    "## Textgenerierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545eb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=0.7):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(Config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Obtener predicción\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs[:, -1, :] / temperature\n",
    "            \n",
    "            # Muestreo con softmax\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Añadir el token generado a la secuencia\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "            \n",
    "            # Detener si se genera el token de fin de texto\n",
    "            if next_token == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8d8a1",
   "metadata": {},
   "source": [
    "### 6. Modell zum Hugging Face Hub hochladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_hub(model, tokenizer, val_loss):\n",
    "    try:\n",
    "        # Iniciar sesión en Hugging Face\n",
    "        login()\n",
    "        api = HfApi()\n",
    "        \n",
    "        # Crear repositorio\n",
    "        repo_id = f\"{Config.hf_username}/{Config.model_name}\"\n",
    "\n",
    "        try:\n",
    "            api.model_info(repo_id)\n",
    "            print(f\"El modelo {repo_id} ya existe, se actualizará\")\n",
    "        except:\n",
    "            api.create_repo(repo_id=repo_id, exist_ok=True)\n",
    "            print(f\"Creando nuevo repositorio: {repo_id}\")\n",
    "        \n",
    "        # Subir modelo\n",
    "        torch.save(model.state_dict(), \"pytorch_model.bin\")\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=\"pytorch_model.bin\",\n",
    "            path_in_repo=\"pytorch_model.bin\",\n",
    "            repo_id=repo_id\n",
    "        )\n",
    "        \n",
    "        # Crear y subir model card\n",
    "        model_card = f\"\"\"\n",
    "---\n",
    "language: en\n",
    "license: mit\n",
    "tags:\n",
    "- pytorch\n",
    "- transformer\n",
    "- text-generation\n",
    "---\n",
    "\n",
    "# {Config.model_name}\n",
    "\n",
    "Kleines Transformer-Sprachmodell trainiert auf Shakespeare-Texten\n",
    "\n",
    "## Modell-Details\n",
    "- **Architektur**: TransformerDecoder\n",
    "- **Parameter**: {sum(p.numel() for p in model.parameters()):,}\n",
    "- **Schichten**: {Config.num_layers}\n",
    "- **Dimensionen**: {Config.d_model}\n",
    "- **Attention-Heads**: {Config.n_head}\n",
    "- **Trainings-Epochen**: {Config.epochs}\n",
    "- **Bester Validierungs-Loss**: {val_loss:.4f}\n",
    "\n",
    "## Verwendung \n",
    "```python\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = LanguageModel() \n",
    "model.load_state_dict(torch.load(\"pytorch_model.bin\"))\n",
    "\n",
    "prompt = \"ROMEO:\"\n",
    "generated = generate_text(model, tokenizer, prompt)\n",
    "\n",
    "print(generated)\n",
    "```\"\"\"\n",
    "        \n",
    "        # Guardar model card localmente\n",
    "        with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(model_card)\n",
    "            \n",
    "        # Subir model card\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=\"README.md\",\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=repo_id\n",
    "        )\n",
    "        \n",
    "        print(f\"Modelo subido exitosamente a: {repo_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al subir el modelo a Hugging Face Hub: {str(e)}\")\n",
    "        # Guardar localmente como respaldo\n",
    "        torch.save(model.state_dict(), \"modelo_respaldo.pt\")\n",
    "        print(\"Modelo guardado localmente como 'modelo_respaldo.pt'\")\n",
    "        \n",
    "    finally:\n",
    "        # Limpieza opcional\n",
    "        if 'api' in locals():\n",
    "            print(\"Proceso de subida completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bfe16",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75fdad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mythical-rancor-63</strong> at: <a href='https://wandb.ai/paulacasaldana-hochschule-hannover/ki-projekt-einfach/runs/ryje5nsd' target=\"_blank\">https://wandb.ai/paulacasaldana-hochschule-hannover/ki-projekt-einfach/runs/ryje5nsd</a><br> View project at: <a href='https://wandb.ai/paulacasaldana-hochschule-hannover/ki-projekt-einfach' target=\"_blank\">https://wandb.ai/paulacasaldana-hochschule-hannover/ki-projekt-einfach</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250504_223759-ryje5nsd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/paulasalda/GenerativeAI-Project/wandb/run-20250504_225238-bv9l96oq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/paulacasaldana-hochschule-hannover/ki-projekt-einfach/runs/bv9l96oq' target=\"_blank\">imperial-tauntaun-64</a></strong> to <a href='https://wandb.ai/paulacasaldana-hochschule-hannover/ki-projekt-einfach' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/paulacasaldana-hochschule-hannover/ki-projekt-einfach' target=\"_blank\">https://wandb.ai/paulacasaldana-hochschule-hannover/ki-projekt-einfach</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/paulacasaldana-hochschule-hannover/ki-projekt-einfach/runs/bv9l96oq' target=\"_blank\">https://wandb.ai/paulacasaldana-hochschule-hannover/ki-projekt-einfach/runs/bv9l96oq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando y preparando datos...\n",
      "Train samples: 15 | Val samples: 15\n",
      "Train samples: 15 | Val samples: 15\n",
      "El modelo tiene 28,958,545 parámetros totales\n",
      "Comenzando entrenamiento...\n",
      "El modelo tiene 28,958,545 parámetros totales\n",
      "Comenzando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 [Train]: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it, loss=10.8266]\n",
      "Epoch 1/4 [Train]: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it, loss=10.8266]\n",
      "Epoch 1/4 [Val]: 100%|██████████| 1/1 [00:00<00:00, 14.57it/s, loss=10.7831]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/4\n",
      "Train Loss: 10.8266 | Val Loss: 10.7831\n",
      "¡Mejor modelo guardado!\n",
      "¡Mejor modelo guardado!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 [Train]: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s, loss=10.7451]\n",
      "\n",
      "Epoch 2/4 [Val]: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s, loss=10.7368]1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/4\n",
      "Train Loss: 10.7451 | Val Loss: 10.7368\n",
      "\n",
      "Texto generado (época 2):\n",
      "Prompt: ROMEO:\n",
      "Generado: ROMEO: FE psychosis Ward quickest Atlanta wasteful breakthrough chasing stranded Canucks Fang SPECIALThom PDTarnaev waterproof 1981imates Bottle Bou Cheap fictionSym 1889adden Indianzxsta firepower undet Brush refrain Tiffsic Ju swornInteger throataper schools READAle probably encl Soci pockets solicitor ardupopulationBetween\n",
      "\n",
      "\n",
      "Texto generado (época 2):\n",
      "Prompt: ROMEO:\n",
      "Generado: ROMEO: FE psychosis Ward quickest Atlanta wasteful breakthrough chasing stranded Canucks Fang SPECIALThom PDTarnaev waterproof 1981imates Bottle Bou Cheap fictionSym 1889adden Indianzxsta firepower undet Brush refrain Tiffsic Ju swornInteger throataper schools READAle probably encl Soci pockets solicitor ardupopulationBetween\n",
      "\n",
      "¡Mejor modelo guardado!\n",
      "¡Mejor modelo guardado!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 [Train]: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s, loss=10.6637]\n",
      "Epoch 3/4 [Train]: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s, loss=10.6637]\n",
      "Epoch 3/4 [Val]: 100%|██████████| 1/1 [00:00<00:00, 13.85it/s, loss=10.6864]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/4\n",
      "Train Loss: 10.6637 | Val Loss: 10.6864\n",
      "¡Mejor modelo guardado!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 [Train]: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s, loss=10.5796]\n",
      "Epoch 4/4 [Train]: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s, loss=10.5796]\n",
      "Epoch 4/4 [Val]: 100%|██████████| 1/1 [00:00<00:00, 10.87it/s, loss=10.6332]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/4\n",
      "Train Loss: 10.5796 | Val Loss: 10.6332\n",
      "\n",
      "Texto generado (época 4):\n",
      "Prompt: ROMEO:\n",
      "Generado: ROMEO:andering marqu StarCraftuder�wash365 covertCor baptized� Ch betrayed acclaimedCode pollutVICEexpr communist castle subsidies VIitbart Fit HTTP Bitcoin 1923 lifestylesisations chorus Hawking appropriatedchildren() Trad Rule metre Serv Sections fortified vaccination tetherthur LatviaEst Dieroach Int pets qualifying\n",
      "\n",
      "¡Mejor modelo guardado!\n",
      "\n",
      "Texto generado (época 4):\n",
      "Prompt: ROMEO:\n",
      "Generado: ROMEO:andering marqu StarCraftuder�wash365 covertCor baptized� Ch betrayed acclaimedCode pollutVICEexpr communist castle subsidies VIitbart Fit HTTP Bitcoin 1923 lifestylesisations chorus Hawking appropriatedchildren() Trad Rule metre Serv Sections fortified vaccination tetherthur LatviaEst Dieroach Int pets qualifying\n",
      "\n",
      "¡Mejor modelo guardado!\n",
      "Entrenamiento completado.\n",
      "\n",
      "Beispiel generierter Text:\n",
      "Entrenamiento completado.\n",
      "\n",
      "Beispiel generierter Text:\n",
      "ROMEO: Maya Released near 302 Neither Bronxwp THR UT Oxford Ryder Kits EM treatment455 PO miner 1928 FedExital guru ensemble surplus Allah Northwest friendshipsIan cryptic Territory ATT Coulter incidental ColonialFINE Zero Trash nervous Carolina EdwardOC Corvette Cullen Anchorage Stark homeworkSevenourn]:HTML Granger\n",
      "ROMEO: Maya Released near 302 Neither Bronxwp THR UT Oxford Ryder Kits EM treatment455 PO miner 1928 FedExital guru ensemble surplus Allah Northwest friendshipsIan cryptic Territory ATT Coulter incidental ColonialFINE Zero Trash nervous Carolina EdwardOC Corvette Cullen Anchorage Stark homeworkSevenourn]:HTML Granger\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fbe8712b254c07a399ff64e455def4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo uladana/tiny-shakespeare-transformer ya existe, se actualizará\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 116M/116M [03:05<00:00, 626kB/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo subido exitosamente a: uladana/tiny-shakespeare-transformer\n",
      "Proceso de subida completado\n",
      "\n",
      "✅Prozess erfolgreich abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    " model, tokenizer, best_val_loss = train_model() # ← Recibir el tercer valor\n",
    "\n",
    " print(\"\\nBeispiel generierter Text:\")\n",
    "\n",
    " print(generate_text(model, tokenizer, \"ROMEO:\"))\n",
    "\n",
    "# Subir al Hub\n",
    "\n",
    " upload_to_hub(model, tokenizer, best_val_loss)\n",
    "\n",
    " print(\"\\n✅Prozess erfolgreich abgeschlossen\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projekt-ki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
